"""
MetaLife OS - í†µí•© AI ì—ì´ì „íŠ¸ ì½”ì–´ (ìˆ˜ì •ëœ ë²„ì „)
Agent_Localê³¼ Agentì˜ ê¸°ëŠ¥ì„ ê²°í•©í•œ í•˜ì´ë¸Œë¦¬ë“œ AI ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ
"""

from abc import ABC, abstractmethod
from typing import Dict, List, Any, Optional, Union, Tuple
from enum import Enum
import asyncio
import json
import logging
import time
from dataclasses import dataclass, field
from pathlib import Path

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class AgentType(Enum):
    LOCAL = "local"  # Agent_Local: 100% ë¡œì»¬ ì²˜ë¦¬
    GLM = "glm"  # Agent: GLM-4.7 ê¸°ë°˜ ì½”ë“œ ìƒì„±
    HYBRID = "hybrid"  # í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“œ
    CLOUD = "cloud"  # í´ë¼ìš°ë“œ API ìš°ì„ 


class TaskType(Enum):
    CODE_GENERATION = "code_generation"
    WEB_BROWSING = "web_browsing"
    CONTENT_CREATION = "content_creation"
    FILE_MANAGEMENT = "file_management"
    RESEARCH = "research"
    AUTOMATION = "automation"


@dataclass
class AgentTask:
    """AI ì—ì´ì „íŠ¸ íƒœìŠ¤í¬ ì •ì˜"""

    id: str
    type: TaskType
    description: str
    context: Dict[str, Any]
    priority: int = 1
    agent_type: Optional[AgentType] = None
    tools: List[str] = field(default_factory=list)


@dataclass
class AgentResponse:
    """AI ì—ì´ì „íŠ¸ ì‘ë‹µ ì •ì˜"""

    task_id: str
    success: bool
    content: str
    metadata: Dict[str, Any]
    error: Optional[str] = None
    execution_time: float = 0.0
    tokens_used: int = 0


class BaseProvider(ABC):
    """LLM ì œê³µì ê¸°ë³¸ í´ë˜ìŠ¤"""

    @abstractmethod
    async def generate(self, prompt: str, **kwargs) -> str:
        pass

    @abstractmethod
    async def generate_with_tools(
        self, prompt: str, tools: List[Dict]
    ) -> Tuple[str, List[Dict]]:
        pass

    @abstractmethod
    def get_model_info(self) -> Dict[str, Any]:
        pass


class OpenAIProvider(BaseProvider):
    """OpenAI API ì œê³µì"""

    def __init__(self, api_key: str, model: str = "gpt-4"):
        self.api_key = api_key
        self.model = model
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” openai ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸

    async def generate(self, prompt: str, **kwargs) -> str:
        # OpenAI API í˜¸ì¶œ êµ¬í˜„
        logger.info(f"OpenAI ìƒì„± ìš”ì²­: {len(prompt)} ë¬¸ì")
        # return await openai.ChatCompletion.create(...)
        return f"OpenAI ì‘ë‹µ: {prompt[:50]}..."

    async def generate_with_tools(
        self, prompt: str, tools: List[Dict]
    ) -> Tuple[str, List[Dict]]:
        # íˆ´ ì½œê³¼ í•¨ê»˜ ìƒì„±
        response = await self.generate(prompt)
        tool_calls = []  # íˆ´ ì½œ íŒŒì‹±
        return response, tool_calls

    def get_model_info(self) -> Dict[str, Any]:
        return {
            "provider": "openai",
            "model": self.model,
            "capabilities": ["text", "code", "tools", "vision"],
        }


class OllamaProvider(BaseProvider):
    """Ollama ë¡œì»¬ LLM ì œê³µì"""

    def __init__(
        self, base_url: str = "http://localhost:11434", model: str = "deepseek-r1:14b"
    ):
        self.base_url = base_url
        self.model = model

    async def generate(self, prompt: str, **kwargs) -> str:
        # Ollama API í˜¸ì¶œ êµ¬í˜„
        logger.info(f"Ollama ìƒì„± ìš”ì²­: {self.model}")
        # return await requests.post(f"{self.base_url}/api/generate", ...)
        return f"Ollama({self.model}) ì‘ë‹µ: {prompt[:50]}..."

    async def generate_with_tools(
        self, prompt: str, tools: List[Dict]
    ) -> Tuple[str, List[Dict]]:
        response = await self.generate(prompt)
        tool_calls = []
        return response, tool_calls

    def get_model_info(self) -> Dict[str, Any]:
        return {
            "provider": "ollama",
            "model": self.model,
            "capabilities": ["text", "code"],
            "local": True,
        }


class GLMProvider(BaseProvider):
    """GLM-4.7 ì œê³µì (Z.ai)"""

    def __init__(self, api_key: str, base_url: str = "https://api.z.ai/api/paas/v4"):
        self.api_key = api_key
        self.base_url = base_url
        self.model = "glm-4.7"

    async def generate(self, prompt: str, **kwargs) -> str:
        logger.info(f"GLM-4.7 ìƒì„± ìš”ì²­")
        # GLM API í˜¸ì¶œ êµ¬í˜„
        return f"GLM-4.7 ì‘ë‹µ: {prompt[:50]}..."

    async def generate_with_tools(
        self, prompt: str, tools: List[Dict]
    ) -> Tuple[str, List[Dict]]:
        response = await self.generate(prompt)
        tool_calls = []
        return response, tool_calls

    def get_model_info(self) -> Dict[str, Any]:
        return {
            "provider": "glm",
            "model": self.model,
            "capabilities": ["text", "code", "tools"],
        }


class BaseTool(ABC):
    """ì—ì´ì „íŠ¸ íˆ´ ê¸°ë³¸ í´ë˜ìŠ¤"""

    @property
    @abstractmethod
    def name(self) -> str:
        pass

    @property
    @abstractmethod
    def description(self) -> str:
        pass

    @abstractmethod
    async def execute(self, **kwargs) -> Dict[str, Any]:
        pass


class WebBrowserTool(BaseTool):
    """ì›¹ ë¸Œë¼ìš°ì§• íˆ´ (Agent_Local ê¸°ë°˜)"""

    def __init__(self, headless: bool = True, stealth: bool = True):
        self.headless = headless
        self.stealth = stealth

    @property
    def name(self) -> str:
        return "web_browser"

    @property
    def description(self) -> str:
        return "ì›¹ì‚¬ì´íŠ¸ë¥¼ íƒìƒ‰í•˜ê³  ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤"

    async def execute(self, **kwargs) -> Dict[str, Any]:
        url = kwargs.get("url")
        search_query = kwargs.get("search_query")

        if search_query:
            # SearXNGìœ¼ë¡œ ê²€ìƒ‰
            return await self._search(search_query)
        elif url:
            # íŠ¹ì • URL ë°©ë¬¸
            return await self._visit_page(url)
        else:
            return {"error": "URL ë˜ëŠ” ê²€ìƒ‰ì–´ê°€ í•„ìš”í•©ë‹ˆë‹¤"}

    async def _search(self, query: str) -> Dict[str, Any]:
        # SearXNG ê²€ìƒ‰ êµ¬í˜„
        logger.info(f"ê²€ìƒ‰: {query}")
        return {
            "success": True,
            "results": [
                {
                    "title": "ì˜ˆì œ ê²°ê³¼",
                    "url": "https://example.com",
                    "snippet": "ê²€ìƒ‰ ê²°ê³¼ ìŠ¤ë‹ˆí«...",
                }
            ],
            "query": query,
        }

    async def _visit_page(self, url: str) -> Dict[str, Any]:
        # Selenium ë¸Œë¼ìš°ì € ìë™í™” êµ¬í˜„
        logger.info(f"í˜ì´ì§€ ë°©ë¬¸: {url}")
        return {
            "success": True,
            "url": url,
            "content": "í˜ì´ì§€ ë‚´ìš©...",
            "title": "í˜ì´ì§€ ì œëª©",
        }


class CodeGenerationTool(BaseTool):
    """ì½”ë“œ ìƒì„± íˆ´ (Agent ê¸°ë°˜)"""

    def __init__(self, provider: BaseProvider):
        self.provider = provider

    @property
    def name(self) -> str:
        return "code_generation"

    @property
    def description(self) -> str:
        return "ë‹¤ì–‘í•œ ì–¸ì–´ì˜ ì½”ë“œë¥¼ ìƒì„±í•˜ê³  ìˆ˜ì •í•©ë‹ˆë‹¤"

    async def execute(self, **kwargs) -> Dict[str, Any]:
        language = kwargs.get("language", "python")
        description = kwargs.get("description", "")
        file_path = kwargs.get("file_path")

        prompt = f"""
        ì–¸ì–´: {language}
        ìš”êµ¬ì‚¬í•­: {description}
        
        ì™„ì „í•œ ì½”ë“œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”. ì£¼ì„ê³¼ ì˜ˆì œë¥¼ í¬í•¨í•´ì£¼ì„¸ìš”.
        """

        code = await self.provider.generate(prompt)

        if file_path:
            # íŒŒì¼ ì €ì¥ ë¡œì§
            Path(file_path).parent.mkdir(parents=True, exist_ok=True)
            with open(file_path, "w", encoding="utf-8") as f:
                f.write(code)

        return {
            "success": True,
            "language": language,
            "code": code,
            "file_path": file_path,
        }


class GitHubTool(BaseTool):
    """GitHub ìë™í™” íˆ´"""

    def __init__(self, token: str):
        self.token = token

    @property
    def name(self) -> str:
        return "github"

    @property
    def description(self) -> str:
        return "GitHub ì €ì¥ì†Œë¥¼ ê´€ë¦¬í•˜ê³  PRì„ ìƒì„±í•©ë‹ˆë‹¤"

    async def execute(self, **kwargs) -> Dict[str, Any]:
        action = kwargs.get("action")

        if action == "create_pr":
            return await self._create_pr(**kwargs)
        elif action == "create_issue":
            return await self._create_issue(**kwargs)
        else:
            return {"error": f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì•¡ì…˜: {action}"}

    async def _create_pr(
        self,
        title: str = "",
        body: str = "",
        head: str = "",
        base: str = "main",
        **kwargs,
    ) -> Dict[str, Any]:
        # GitHub PR ìƒì„± êµ¬í˜„
        logger.info(f"PR ìƒì„±: {title}")
        return {
            "success": True,
            "pr_url": "https://github.com/example/repo/pull/1",
            "title": title,
            "number": 1,
        }

    async def _create_issue(
        self, title: str = "", body: str = "", **kwargs
    ) -> Dict[str, Any]:
        # GitHub Issue ìƒì„± êµ¬í˜„
        logger.info(f"Issue ìƒì„±: {title}")
        return {
            "success": True,
            "issue_url": "https://github.com/example/repo/issues/1",
            "title": title,
            "number": 1,
        }


class MetaLifeAgent:
    """í†µí•© AI ì—ì´ì „íŠ¸ ë©”ì¸ í´ë˜ìŠ¤"""

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.providers: Dict[str, BaseProvider] = {}
        self.tools: Dict[str, BaseTool] = {}
        self.task_queue = asyncio.Queue()
        self.running = False

        # ì œê³µì ì´ˆê¸°í™”
        self._initialize_providers()

        # íˆ´ ì´ˆê¸°í™”
        self._initialize_tools()

    def _initialize_providers(self):
        """LLM ì œê³µì ì´ˆê¸°í™”"""

        # OpenAI (í´ë¼ìš°ë“œ)
        if self.config.get("openai_api_key"):
            self.providers["openai"] = OpenAIProvider(
                api_key=self.config["openai_api_key"],
                model=self.config.get("openai_model", "gpt-4"),
            )

        # Ollama (ë¡œì»¬)
        if self.config.get("ollama_enabled", True):
            self.providers["ollama"] = OllamaProvider(
                base_url=self.config.get("ollama_url", "http://localhost:11434"),
                model=self.config.get("ollama_model", "deepseek-r1:14b"),
            )

        # GLM-4.7 (ì½”ë“œ ìƒì„± íŠ¹í™”)
        if self.config.get("zai_api_key"):
            self.providers["glm"] = GLMProvider(api_key=self.config["zai_api_key"])

    def _initialize_tools(self):
        """íˆ´ ì´ˆê¸°í™”"""

        # ì›¹ ë¸Œë¼ìš°ì§•
        self.tools["web_browser"] = WebBrowserTool(
            headless=self.config.get("headless_browser", True),
            stealth=self.config.get("stealth_mode", True),
        )

        # ì½”ë“œ ìƒì„± (GLM ì œê³µì ì‚¬ìš©)
        if "glm" in self.providers:
            self.tools["code_generation"] = CodeGenerationTool(self.providers["glm"])

        # GitHub ìë™í™”
        if self.config.get("github_token"):
            self.tools["github"] = GitHubTool(self.config["github_token"])

    async def process_task(self, task: AgentTask) -> AgentResponse:
        """íƒœìŠ¤í¬ ì²˜ë¦¬ ë©”ì¸ ë¡œì§"""
        start_time = time.time()

        try:
            # ì ì ˆí•œ ì œê³µì ì„ íƒ
            provider = self._select_provider(task)

            if provider is None:
                return AgentResponse(
                    task_id=task.id,
                    success=False,
                    content="",
                    metadata={},
                    error="ì‚¬ìš© ê°€ëŠ¥í•œ LLM ì œê³µìê°€ ì—†ìŠµë‹ˆë‹¤",
                    execution_time=time.time() - start_time,
                )

            # í•„ìš”í•œ íˆ´ ì„ íƒ
            available_tools = [
                self.tools[tool_name]
                for tool_name in task.tools
                if tool_name in self.tools
            ]

            # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            prompt = self._build_prompt(task)

            if available_tools:
                # íˆ´ê³¼ í•¨ê»˜ ìƒì„±
                tool_schemas = [self._tool_to_schema(tool) for tool in available_tools]
                content, tool_calls = await provider.generate_with_tools(
                    prompt, tool_schemas
                )

                # íˆ´ ì‹¤í–‰
                tool_results = []
                for tool_call in tool_calls:
                    tool_name = tool_call.get("function", {}).get("name")
                    if tool_name in self.tools:
                        result = await self.tools[tool_name].execute(
                            **tool_call.get("function", {}).get("arguments", {})
                        )
                        tool_results.append(result)

                metadata = {
                    "tool_calls": len(tool_calls),
                    "tool_results": tool_results,
                    "provider": provider.get_model_info(),
                }
            else:
                # ì¼ë°˜ ìƒì„±
                content = await provider.generate(prompt)
                metadata = {"provider": provider.get_model_info()}

            execution_time = time.time() - start_time

            return AgentResponse(
                task_id=task.id,
                success=True,
                content=content,
                metadata=metadata,
                execution_time=execution_time,
            )

        except Exception as e:
            logger.error(f"íƒœìŠ¤í¬ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return AgentResponse(
                task_id=task.id,
                success=False,
                content="",
                metadata={},
                error=str(e),
                execution_time=time.time() - start_time,
            )

    def _select_provider(self, task: AgentTask) -> Optional[BaseProvider]:
        """íƒœìŠ¤í¬ì— ì í•©í•œ ì œê³µì ì„ íƒ"""

        # ì½”ë“œ ìƒì„±ì€ GLM ìš°ì„ 
        if task.type == TaskType.CODE_GENERATION and "glm" in self.providers:
            return self.providers["glm"]

        # ì—ì´ì „íŠ¸ íƒ€ì… ëª…ì‹œì  ì§€ì •
        if task.agent_type == AgentType.LOCAL and "ollama" in self.providers:
            return self.providers["ollama"]
        elif task.agent_type == AgentType.CLOUD and "openai" in self.providers:
            return self.providers["openai"]

        # ê¸°ë³¸ ì „ëµ: ë¡œì»¬ ìš°ì„ 
        return (
            self.providers.get("ollama")
            or self.providers.get("openai")
            or self.providers.get("glm")
        )

    def _build_prompt(self, task: AgentTask) -> str:
        """íƒœìŠ¤í¬ ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ êµ¬ì„±"""

        base_prompt = f"""ë‹¹ì‹ ì€ MetaLife OSì˜ í†µí•© AI ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤.

íƒœìŠ¤í¬ ìœ í˜•: {task.type.value}
ì„¤ëª…: {task.description}

ì»¨í…ìŠ¤íŠ¸: {json.dumps(task.context, ensure_ascii=False, indent=2)}

ìš”ì²­ì‚¬í•­ì„ ì™„ìˆ˜í•˜ê¸° ìœ„í•œ êµ¬ì²´ì ì¸ ë‹¨ê³„ì™€ ê²°ê³¼ë¬¼ì„ ì œê³µí•´ì£¼ì„¸ìš”.
"""
        return base_prompt

    def _tool_to_schema(self, tool: BaseTool) -> Dict[str, Any]:
        """íˆ´ì„ ìŠ¤í‚¤ë§ˆë¡œ ë³€í™˜"""
        return {
            "type": "function",
            "function": {
                "name": tool.name,
                "description": tool.description,
                "parameters": {"type": "object", "properties": {}, "required": []},
            },
        }

    async def run_chat_mode(self):
        """ì¸í„°ë™í‹°ë¸Œ ì±„íŒ… ëª¨ë“œ"""
        print("ğŸ¤– MetaLife OS AI ì—ì´ì „íŠ¸ ëŒ€í™” ëª¨ë“œ")
        print("ì¢…ë£Œí•˜ë ¤ë©´ 'quit', 'exit', 'ì¢…ë£Œ'ë¥¼ ì…ë ¥í•˜ì„¸ìš”")
        print("-" * 50)

        self.running = True

        while self.running:
            try:
                user_input = input("\nğŸ’¬ ì…ë ¥: ").strip()

                if user_input.lower() in ["quit", "exit", "ì¢…ë£Œ"]:
                    print("ğŸ‘‹ ì•ˆë…•íˆ ê°€ì„¸ìš”!")
                    break

                # íƒœìŠ¤í¬ ìƒì„±
                task = AgentTask(
                    id=f"chat_{int(time.time())}",
                    type=TaskType.RESEARCH,  # ê¸°ë³¸ ë¦¬ì„œì¹˜ íƒ€ì…
                    description=user_input,
                    context={"mode": "chat"},
                    tools=["web_browser"],  # ê¸°ë³¸ ì›¹ ë¸Œë¼ìš°ì§• í™œì„±í™”
                )

                print("ğŸ”„ ì²˜ë¦¬ ì¤‘...")
                response = await self.process_task(task)

                if response.success:
                    print(f"\nğŸ¤– ì‘ë‹µ ({response.execution_time:.2f}ì´ˆ):")
                    print(response.content)

                    if response.metadata.get("tool_results"):
                        print("\nğŸ”§ íˆ´ ì‹¤í–‰ ê²°ê³¼:")
                        for result in response.metadata["tool_results"]:
                            print(f"  - {result}")
                else:
                    print(f"\nâŒ ì˜¤ë¥˜: {response.error}")

            except KeyboardInterrupt:
                print("\nğŸ‘‹ ì•ˆë…•íˆ ê°€ì„¸ìš”!")
                break
            except Exception as e:
                print(f"\nâŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")

    async def start_worker(self):
        """ë°±ê·¸ë¼ìš´ë“œ ì›Œì»¤ ëª¨ë“œ"""
        print("ğŸ”„ MetaLife OS ë°±ê·¸ë¼ìš´ë“œ ì›Œì»¤ ì‹œì‘")
        self.running = True

        while self.running:
            try:
                # íì—ì„œ íƒœìŠ¤í¬ ëŒ€ê¸°
                task = await asyncio.wait_for(self.task_queue.get(), timeout=1.0)

                print(f"ğŸ“ íƒœìŠ¤í¬ ì²˜ë¦¬: {task.description}")
                response = await self.process_task(task)

                if response.success:
                    print(f"âœ… íƒœìŠ¤í¬ ì™„ë£Œ: {response.execution_time:.2f}ì´ˆ")
                else:
                    print(f"âŒ íƒœìŠ¤í¬ ì‹¤íŒ¨: {response.error}")

                self.task_queue.task_done()

            except asyncio.TimeoutError:
                continue
            except Exception as e:
                print(f"âŒ ì›Œì»¤ ì˜¤ë¥˜: {e}")

    def add_task(self, task: AgentTask):
        """íƒœìŠ¤í¬ íì— ì¶”ê°€"""
        asyncio.create_task(self.task_queue.put(task))

    def stop(self):
        """ì—ì´ì „íŠ¸ ì¤‘ì§€"""
        self.running = False


# í¸ì˜ í•¨ìˆ˜
def create_agent(config_file: str = "config.json") -> MetaLifeAgent:
    """ì„¤ì • íŒŒì¼ë¡œ ì—ì´ì „íŠ¸ ìƒì„±"""
    try:
        with open(config_file, "r", encoding="utf-8") as f:
            config = json.load(f)
    except FileNotFoundError:
        # ê¸°ë³¸ ì„¤ì •
        config = {
            "ollama_enabled": True,
            "headless_browser": True,
            "stealth_mode": True,
        }

    return MetaLifeAgent(config)


async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse
    import os

    parser = argparse.ArgumentParser(description="MetaLife OS AI ì—ì´ì „íŠ¸")
    parser.add_argument("--mode", choices=["chat", "worker"], default="chat")
    parser.add_argument("--config", default="agent_config.json")
    args = parser.parse_args()

    # í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
    config = {
        "openai_api_key": os.getenv("OPENAI_API_KEY"),
        "zai_api_key": os.getenv("ZAI_API_KEY"),
        "github_token": os.getenv("GITHUB_TOKEN"),
        "ollama_enabled": os.getenv("OLLAMA_ENABLED", "true").lower() == "true",
        "headless_browser": os.getenv("HEADLESS_BROWSER", "true").lower() == "true",
        "stealth_mode": os.getenv("STEALTH_MODE", "true").lower() == "true",
        "ollama_url": os.getenv("OLLAMA_URL", "http://localhost:11434"),
        "ollama_model": os.getenv("OLLAMA_MODEL", "deepseek-r1:14b"),
    }

    agent = MetaLifeAgent(config)

    if args.mode == "chat":
        await agent.run_chat_mode()
    else:
        await agent.start_worker()


if __name__ == "__main__":
    asyncio.run(main())
